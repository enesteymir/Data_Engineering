{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592d42c6-986b-47d0-8dfc-06a7c060594d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_sub, current_date, to_date, date_format, when, min, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Window\n",
    "from config import get_currency_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a3a2e8-5ee5-4d3f-ad9d-0da17a2cc4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"env\", \"test\", \"\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "currency_source_path, currency_target_path, currency_checkpoint = get_currency_path(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7037039c-601d-4454-a6a6-93f5072e58ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the currency data in bulk\n",
    "currency_bulk_df = (\n",
    "    spark.read\n",
    "        .format(\"delta\")  \n",
    "        .load(currency_source_path)\n",
    "        .select(\n",
    "                to_date(col('dt'), \"yyyy-MM-dd\").alias(\"date\") \n",
    "                ,col(\"data\")\n",
    "                )\n",
    "        .filter(col(\"date\") >= date_sub(current_date(), 30) ) # Update filter accordingly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50e14940-efbe-4364-9675-8d8fbc83dbe2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Fill the historical missed data for the weekends , by copying related Fridays's data value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf291f7-740d-4da8-aa1d-1dacb81e303c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the day of the week \n",
    "currency_bulk_df = currency_bulk_df.withColumn(\"day_of_week\", date_format(col(\"date\"), \"EEEE\"))\n",
    "\n",
    "# Filter for Fridays\n",
    "friday_df = currency_bulk_df.filter(col(\"day_of_week\") == \"Friday\").drop(\"day_of_week\")  # Drop day_of_week from Friday data to avoid conflict\n",
    "\n",
    "# Get Min and Max dates\n",
    "min_date = currency_bulk_df.agg(min(\"date\").alias(\"min_date\")).collect()[0][\"min_date\"]\n",
    "max_date = currency_bulk_df.agg(max(\"date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# Generate a DataFrame with all dates between min and max date (including weekends)\n",
    "date_range_df = spark.sql(f\"SELECT explode(sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day)) as date\")\n",
    "\n",
    "# Identify missing dates (Saturdays and Sundays) not present in Delta data\n",
    "existing_dates_df = currency_bulk_df.select(\"date\").distinct()\n",
    "missing_dates_df = date_range_df.join(existing_dates_df, \"date\", \"leftanti\")\n",
    "\n",
    "# Identify missing Saturdays and Sundays\n",
    "missing_weekends_df = missing_dates_df.withColumn(\"day_of_week\", date_format(col(\"date\"), \"EEEE\")) \\\n",
    "                                      .filter((col(\"day_of_week\") == \"Saturday\") | (col(\"day_of_week\") == \"Sunday\"))\n",
    "\n",
    "# Find the corresponding Friday's data for each missing weekend date\n",
    "weekend_filled_df = missing_weekends_df \\\n",
    "    .withColumn(\"related_friday\", date_sub(col(\"date\"), when(col(\"day_of_week\") == \"Saturday\", 1).otherwise(2))) \\\n",
    "    .join(friday_df.withColumnRenamed(\"date\", \"friday_date\"), col(\"related_friday\") == col(\"friday_date\"), \"left\") \\\n",
    "    .drop(\"related_friday\", \"day_of_week\", \"friday_date\")  \n",
    "\n",
    "\n",
    "# Drop unnecessary col from the main df and Combine the original data and the new weekend data\n",
    "currency_bulk_df = currency_bulk_df.drop('day_of_week')\n",
    "filled_df = currency_bulk_df.unionByName(weekend_filled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e60dcd-e978-4a39-a7ec-b9ea3de86f90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Insert 1 day data as a buffer by copying the max date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f7fb10-5673-4ff6-a063-c7e420b4b8e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the data field of the max row\n",
    "data_value = filled_df.filter(filled_df.date == max_date).select(\"data\").first()[0]\n",
    "# Create a new date which is one day after max_date\n",
    "new_date = max_date + timedelta(days=1)\n",
    "# Get all rows that have the max_date\n",
    "max_rows = filled_df.filter(filled_df.date == max_date)\n",
    "# Create new rows using the existing struct data\n",
    "new_rows = [(new_date, row.data) for row in max_rows.collect()]\n",
    "# Create a DataFrame for the new rows\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"since\", StringType(), True),\n",
    "        StructField(\"source\", StringType(), True),\n",
    "        StructField(\"source_amount\", DoubleType(), True),\n",
    "        StructField(\"target\", StringType(), True),\n",
    "        StructField(\"target_amount\", DoubleType(), True),\n",
    "        StructField(\"until\", StringType(), True)\n",
    "    ]), True),\n",
    "])\n",
    "new_rows_df = spark.createDataFrame(new_rows, schema)\n",
    "# Union the original DataFrame with the new rows DataFrame\n",
    "final_df = filled_df.union(new_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a851f9-03c0-470b-a4af-2a857e2c8dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#final_df.orderBy(col('date').desc()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4db4138-b3aa-4abd-b083-71a4f7b93a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the final data to a Delta table\n",
    "final_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\")  \\\n",
    "    .option(\"path\", currency_target_path) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CurrencyDailyJob",
   "widgets": {
    "env": {
     "currentValue": "test",
     "nuid": "2f37838c-1ac6-4382-a1ae-dd9feb340791",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "test",
      "label": "",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "test",
      "label": "",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
