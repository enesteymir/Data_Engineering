{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3c0062-6743-4bf0-bcee-fb24333c0e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, explode, transform\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.utils\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a3f8a95-929a-4d6c-9ca5-2bef60ca8d84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_path = \"s3a://saiki-datalake-eu-central-1/data/eventqueue/customer.behaviour.tracking-events.test\" ,\n",
    "source_checkpoint = \"s3a://.....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install s3fs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88f2e1f-87a3-4681-b803-1a24e0e57f6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the schema of the source JSON files\n",
    "import s3fs\n",
    "source_schema = (\n",
    "            spark.read\n",
    "            .format(\"s3selectJSON\")\n",
    "            .json(f\"{source_path}/dt=2024-09-16\")\n",
    "            ).schema\n",
    "\n",
    "# Add dt partition field to the schema         \n",
    "source_schema = source_schema.add(StructField('dt', StringType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream read with schema\n",
    "source_df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .schema(source_schema)\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", source_checkpoint)\n",
    "            .load(f\"{source_path}/dt=2024-09-17\")\n",
    "            .limit(5)\n",
    "        )\n",
    "\n",
    "source_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700bd005-889f-41b1-a9dd-96ba168233ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: consent dtype: struct\n",
      "column: frontend dtype: struct\n",
      "column: metadata dtype: struct\n"
     ]
    }
   ],
   "source": [
    "# Check the cols with struct type\n",
    "for c in source_df.dtypes: \n",
    "    if c[1][:6] == 'struct':\n",
    "        print(\"column:\", c[0], \"dtype:\", c[1][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b63243-d57f-46c5-8ddc-7fc454354779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to be used to flatten all struct type columns\n",
    "def flatten_df(nested_df):\n",
    "    flat_cols = []\n",
    "    nested_cols = []\n",
    "    \n",
    "    # Separate flat columns from nested columns\n",
    "    for column_name, dtype in nested_df.dtypes:\n",
    "        if \".\" not in column_name and dtype.startswith(\"struct\"):\n",
    "            nested_cols.append(column_name)\n",
    "        else:\n",
    "            flat_cols.append(column_name)\n",
    "    \n",
    "    # Select flat columns\n",
    "    selected_cols = [col(column) for column in flat_cols]\n",
    "    \n",
    "    # Unnest the nested columns\n",
    "    for nested_col in nested_cols:\n",
    "            expanded = [col(f\"{nested_col}.{subfield}\").alias(f\"{nested_col}_{subfield}\")\n",
    "                        for subfield in nested_df.select(f\"{nested_col}.*\").columns\n",
    "                       ]\n",
    "       \n",
    "            selected_cols.extend(expanded)         \n",
    "    \n",
    "    # Return the new streaming DataFrame\n",
    "    return nested_df.select(*selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten until all struct dtype fields are unnested\n",
    "flattened_source_df = flatten_df(source_df)\n",
    "flattened_source_df = flatten_df(flattened_source_df)\n",
    "flattened_source_df = flatten_df(flattened_source_df)\n",
    "flattened_source_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d24d06d-3c32-4a5e-856a-bed75657bce9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the array type \"skus\" columns into seperate cols\n",
    "splitted_df = flattened_source_df \\\n",
    "    .select(\n",
    "        '*',\n",
    "        col('frontend_event_params_skus').getItem(0).discount.alias(\"sku_discount\"),\n",
    "        col('frontend_event_params_skus').getItem(0).sku.alias(\"sku\"),\n",
    "        col('frontend_event_params_skus').getItem(0).sku_brand.alias(\"sku_brand\"),\n",
    "        col('frontend_event_params_skus').getItem(0).sku_name.alias(\"sku_name\")\n",
    "    ).drop('frontend_event_params_skus')\n",
    "\n",
    "splitted_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cbf1f3-4aa3-47e2-b392-0b6791d12af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert array type fields to string array\n",
    "def convert_array_to_string_array(df: DataFrame) -> DataFrame:\n",
    " \n",
    "    array_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, ArrayType)]\n",
    "    \n",
    "    for array_col in array_columns:\n",
    "        df = df.withColumn(array_col, transform(col(array_col), lambda x: x.cast(\"string\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af1dab3-f24b-4661-a91e-3f1ac322e831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_df = convert_array_to_string_array(splitted_df)\n",
    "string_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19480ede-5ef4-4fad-9260-c6b5d3cb5d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove prefixes from the column names\n",
    "def clean_column_names(df: DataFrame)-> DataFrame:\n",
    "    new_column_names = [col_name.replace(\"metadata_\",\"\") \\\n",
    "                                .replace(\"frontend_event_params_\",\"\") \\\n",
    "                                .replace(\"frontend_device_\", \"\") \\\n",
    "                                .replace(\"frontend_\", \"\") \\\n",
    "                        for col_name in df.columns]\n",
    "    \n",
    "    cleaned_df = df.toDF(*new_column_names)\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_column_names(string_df)\n",
    "cleaned_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef9d18b-4283-42dd-8857-ba1679b0d22d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create function to rename the columns\n",
    "def rename_columns(df: DataFrame)-> DataFrame:\n",
    "    updated_df = df \\\n",
    "                    .withColumnRenamed(\"shipping_fee_amount\", \"shipping_fee_eur\") \\\n",
    "                    .withColumnRenamed(\"tax_amount\", \"tax_eur\") \n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the cols\n",
    "renamed_df = rename_columns(cleaned_df)\n",
    "renamed_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
