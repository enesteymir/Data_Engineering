{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d4d211-a651-4342-b7c2-1c7e84378cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING\n",
    "    > Remove unnecessary code from notebooks that would return results, such as display and count.\n",
    "    > Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs.\n",
    "    > To help streaming jobs recover automatically, configure jobs with infinite retries.\n",
    "    > Do not use auto-scaling for workloads with Structured Streaming.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a0a2e9-0535-4adb-978a-062f2607520b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.utils\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "enddate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "dates = [startdate,enddate]\n",
    "yesterday = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ee002f-554b-4dc9-8797-dc095dd582d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Streaming with Delta format data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"s3://datalake-binary/event-types/data/shop.tracking.outfit-card.click\"\n",
    "target_path = \"s3://tracking-analytics/stream\"\n",
    "target_checkpoint_path = \"s3://tracking-analytics/checkpoints/target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the latest delta file in source s3 bucket directory\n",
    "dbutils.fs.ls(source_path)[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f52cd84-7b29-4b25-9b6e-383d7af23596",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from source Delta table as Stream\n",
    "source_query = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(source_path)\n",
    "    .filter(f.col(\"dt\").between(*dates))\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068da41c-7562-498f-a195-d05f64cd815b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display() on a streaming DataFrame starts a streaming job.\n",
    "display(source_query, streamName = \"read_stream\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d71911d-aa30-472c-8d99-70cf5111b46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stopping the reading stream currently running\n",
    "for stream in spark.streams.active:   \n",
    "    if stream.name == \"read_stream\":\n",
    "        s = spark.streams.get(stream.id)\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3737c8-529f-49f3-9605-dd747dacc087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Stream to Delta Lake\n",
    "# The \"availableNow=True\" option processes all available data in multiple batches then terminates the query\n",
    "# The processingTime = '60 seconds' option checks evey 1 min\n",
    "# The path of \"checkpointLocation\" should be unique for each writer streaming. \n",
    "\n",
    "target_query =  (\n",
    "    source_query\n",
    "        .withColumn('etl_date',f.lit(now))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", target_checkpoint_path)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  \n",
    "        .trigger(availableNow=True)\n",
    "        #.toTable(table_name)\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2be722-12da-49ef-a277-79a143a494ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check writing stream in specified seconds, if it's still running it will return false\n",
    "target_query.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15610bff-15e4-48d9-98c7-1799714cc6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the target stream if needed\n",
    "target_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3054744-83f8-40f7-ad32-cb3b7cb4e2bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trigger Reading and Writing Streaming in single cell\n",
    "source_query = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(source_path)\n",
    "    .filter(f.col(\"dt\").between(*dates))\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "target_query =  (\n",
    "    source_query\n",
    "        .withColumn('etl_date',f.lit(now))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", target_checkpoint_path)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  \n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194895e0-f1cc-4f41-a81a-348c886dbb4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the data at the Target Delta Lake\n",
    "target_df = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .load(target_path)\n",
    "        .where(f.col(\"etl_date\") == now)\n",
    "        .limit(4)\n",
    "        )\n",
    "\n",
    "target_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target stream delta files\n",
    "files = dbutils.fs.ls(target_path)\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the checkpoints info\n",
    "dbutils.fs.ls(target_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6ffd0b-9da5-4f1e-9a8d-c7814e9bb46e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Streaming with JSON format data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62423025-53b4-4b0c-bef7-8dd5a67c14d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stream reading JSON files with Auto Loader - cloudFiles\n",
    "source_path = \"s3a://datalake-eu-central-1/data/eventqueue/shop.tracking.outfit-card.click\"\n",
    "source_checkpoint = \"s3://tracking-analytics/checkpoints/source\"\n",
    "target_path = \"s3://tracking-analytics/stream\"\n",
    "target_checkpoint = \"s3://tracking-analytics/checkpoints/target\"\n",
    "\n",
    "source_query= (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", source_checkpoint)\n",
    "            .load(f\"{source_path}/dt={yesterday}\")\n",
    "            .limit(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57425cb1-a8fa-4492-9cac-17925b13860d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display the source data, this will trigger a streaming DataFrame\n",
    "display(source_query, streamName = \"json_read_stream\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f240d936-e488-44e3-bf00-d7c9b171b8ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the reading stream currently running\n",
    "for stream in spark.streams.active:   \n",
    "    if stream.name == \"json_read_stream\":\n",
    "        s = spark.streams.get(stream.id)\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger Reading and Writing Streaming in single cell\n",
    "source_query = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", source_checkpoint)\n",
    "            .load(f\"{source_path}/dt={yesterday}\")\n",
    "            .limit(5)\n",
    "    )\n",
    "\n",
    "target_query =  (\n",
    "    source_query\n",
    "        .withColumn('dt',f.lit(yesterday))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", target_checkpoint)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  # Check every 1 min\n",
    "        .trigger(availableNow=True) # Run once and then will stop\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target stream delta files\n",
    "files = dbutils.fs.ls(target_path)\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data at the Target Delta Lake\n",
    "target_df = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .load(target_path)\n",
    "        .where(f.col(\"dt\") == yesterday)\n",
    "        .limit(5)\n",
    "        )\n",
    "\n",
    "target_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stream_Data_Loading",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
