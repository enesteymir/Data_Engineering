{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d4d211-a651-4342-b7c2-1c7e84378cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING\n",
    "    > Remove unnecessary code from notebooks that would return results, such as display and count.\n",
    "    > Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs.\n",
    "    > To help streaming jobs recover automatically, configure jobs with infinite retries.\n",
    "    > Do not use auto-scaling for workloads with Structured Streaming.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a0a2e9-0535-4adb-978a-062f2607520b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.utils\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c72593b-17f1-4950-817d-ea20b3fbd92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024-08-21', '2024-08-21']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startdate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "enddate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "dates = [startdate,enddate]\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ee002f-554b-4dc9-8797-dc095dd582d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Streaming with Delta format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cf6385-5336-438b-84ce-44598da19620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='s3://zalando-datalake-binary/event-types/data/shop.tracking.outfit-card.click/dt=2024-08-21/', name='dt=2024-08-21/', size=0, modificationTime=1724327520246)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the latest delta file in source s3 bucket directory\n",
    "dbutils.fs.ls(\"s3://zalando-datalake-binary/event-types/data/shop.tracking.outfit-card.click\")[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f52cd84-7b29-4b25-9b6e-383d7af23596",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from source Delta table as Stream\n",
    "source_path = \"s3://zalando-datalake-binary/event-types/data/shop.tracking.outfit-card.click\"\n",
    "\n",
    "source_query = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(source_path)\n",
    "    .filter(f.col(\"dt\").between(*dates))\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068da41c-7562-498f-a195-d05f64cd815b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display() on a streaming DataFrame starts a streaming job.\n",
    "display(source_query, streamName = \"read_stream\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d71911d-aa30-472c-8d99-70cf5111b46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stopping the reading stream currently running\n",
    "for stream in spark.streams.active:   \n",
    "    if stream.name == \"read_stream\":\n",
    "        s = spark.streams.get(stream.id)\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3737c8-529f-49f3-9605-dd747dacc087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Stream to Delta Lake\n",
    "# The \"availableNow=True\" option processes all available data in multiple batches then terminates the query\n",
    "# The processingTime = '60 seconds' option checks evey 1 min\n",
    "# The path of \"checkpointLocation\" should be unique for each writer streaming. \n",
    "\n",
    "target_path = \"dbfs:/team-tracking/test_stream\"\n",
    "checkpoint_path = \"dbfs:/team-tracking/checkpoints/test_stream\"\n",
    "\n",
    "target_query =  (\n",
    "    source_query\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  \n",
    "        .trigger(availableNow=True)\n",
    "        #.toTable(table_name)\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2be722-12da-49ef-a277-79a143a494ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check writing stream in specified seconds, if it's still running it will return false\n",
    "target_query.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15610bff-15e4-48d9-98c7-1799714cc6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the target stream if needed\n",
    "target_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3054744-83f8-40f7-ad32-cb3b7cb4e2bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trigger Reading and Writing Streaming in single cell\n",
    "source_query = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(source_path)\n",
    "    .filter(f.col(\"dt\").between(*dates))\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "target_query =  (\n",
    "    source_query\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  \n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194895e0-f1cc-4f41-a81a-348c886dbb4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the data at the Target Delta Lake\n",
    "file_path = \"dbfs:/team-tracking/test_stream\"\n",
    "target_df = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .load(file_path)\n",
    "        .filter(f.col(\"dt\").between(*dates))\n",
    "        .limit(4)\n",
    "        )\n",
    "\n",
    "target_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the checkpoints info\n",
    "#%fs ls\n",
    "dbutils.fs.ls(\"dbfs:/team-tracking/checkpoints/test_stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target stream delta files\n",
    "dbutils.fs.ls(\"dbfs:/team-tracking/test_stream/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6ffd0b-9da5-4f1e-9a8d-c7814e9bb46e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Streaming with JSON format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62423025-53b4-4b0c-bef7-8dd5a67c14d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stream reading JSON files with Auto Loader - cloudFiles\n",
    "source_path = \"s3a://datalake-eu-central-1/data/eventqueue/shop.tracking.outfit-card.click\"\n",
    "source_checkpoint = \"s3://tracking-analytics/checkpoints/source\"\n",
    "\n",
    "json_source= (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", source_checkpoint)\n",
    "        .load(file_path)\n",
    "        .limit(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57425cb1-a8fa-4492-9cac-17925b13860d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display() on a streaming DataFrame starts a streaming job.\n",
    "display(json_source, streamName = \"json_read_stream\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f240d936-e488-44e3-bf00-d7c9b171b8ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the reading stream currently running\n",
    "for stream in spark.streams.active:   \n",
    "    if stream.name == \"json_read_stream\":\n",
    "        s = spark.streams.get(stream.id)\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger Reading and Writing Streaming in single cell\n",
    "source_path = \"s3a://zalando-saiki-datalake-eu-central-1/data/eventqueue/shop.tracking.outfit-card.click\"\n",
    "target_path = \"s3://tracking-analytics/stream\"\n",
    "source_checkpoint = \"s3://tracking-analytics/checkpoints/source\"\n",
    "target_checkpoint = \"s3://tracking-analytics/checkpoints/target\"\n",
    "\n",
    "json_source= (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", source_checkpoint)\n",
    "        .load(source_path)\n",
    "        .limit(5)\n",
    "    )\n",
    "\n",
    "target_query =  (\n",
    "    json_source\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", target_checkpoint)\n",
    "        .option(\"path\", target_path)\n",
    "        .outputMode(\"append\")\n",
    "        #.trigger(processingTime = '60 seconds')  \n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stream_Data_Loading",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
