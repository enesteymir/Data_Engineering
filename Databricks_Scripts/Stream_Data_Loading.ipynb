{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d4d211-a651-4342-b7c2-1c7e84378cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING\n",
    "    > Remove unnecessary code from notebooks that would return results, such as display and count.\n",
    "    > Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs.\n",
    "    > To help streaming jobs recover automatically, configure jobs with infinite retries.\n",
    "    > Do not use auto-scaling for workloads with Structured Streaming.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a0a2e9-0535-4adb-978a-062f2607520b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.utils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c72593b-17f1-4950-817d-ea20b3fbd92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2024-08-20', '2024-08-20']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startdate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "enddate = (datetime.now() - timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "dates = [startdate,enddate]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f52cd84-7b29-4b25-9b6e-383d7af23596",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Delta Lake as Stream\n",
    "file_path = \"s3://zalando-datalake-binary/event-types/data/shop.tracking.outfit-card.click\"\n",
    "checkpoint_for_reading = \"/team-tracking/checkpoints/read\"\n",
    "\n",
    "raw_stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_for_reading)\n",
    "    .load(file_path)\n",
    "    .filter(f.col(\"dt\").between(*dates))\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068da41c-7562-498f-a195-d05f64cd815b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display() on a streaming DataFrame starts a streaming job.\n",
    "display(raw_stream_df, streamName = \"read_stream\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d71911d-aa30-472c-8d99-70cf5111b46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stopping the reading stream currently running\n",
    "for stream in spark.streams.active:   \n",
    "    if stream.name == \"read_stream\":\n",
    "        s = spark.streams.get(stream.id)\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3737c8-529f-49f3-9605-dd747dacc087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Stream to Delta Lake\n",
    "# The \"availableNow=True\" option processes all available data in multiple batches then terminates the query\n",
    "# The processingTime = '60 seconds' option checks evey 1 min\n",
    "# The \"checkpointLocation\" should be unique for each streaming writer.It tracks all records processed and state information.\n",
    "\n",
    "target_path = \"/team-tracking/test_stream\"\n",
    "checkpoint_for_writing = \"/team-tracking/checkpoints/write\"\n",
    "\n",
    "write_stream =  (\n",
    "raw_stream_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_for_writing)\n",
    "    .option(\"path\", target_path)\n",
    "    .outputMode(\"append\")\n",
    "    #.trigger(processingTime = '60 seconds')  \n",
    "    .trigger(availableNow=True)\n",
    "    #.toTable(table_name)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2be722-12da-49ef-a277-79a143a494ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check writing stream in specified seconds, if it's still running it will return false\n",
    "write_stream.awaitTermination(timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15610bff-15e4-48d9-98c7-1799714cc6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the writing stream\n",
    "write_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd62f3a-fc8b-448e-9dc5-f6f8306ef63d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the data from the Target Delta Lake\n",
    "file_path = \"/team-tracking/test_stream\"\n",
    "target_df = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .load(file_path)\n",
    "        .filter(f.col(\"dt\").between(*dates))\n",
    "        .limit(4)\n",
    "        )\n",
    "\n",
    "target_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_loading_example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
